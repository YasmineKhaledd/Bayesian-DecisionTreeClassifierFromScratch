{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('D:\\Bank_dataset.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = float(input(\"Enter the percentage of the data to use for training: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets based on the specified percentage\n",
    "train_df = df.sample(frac=percentage)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature names\n",
    "features = ['age', 'job', 'marital', 'education', 'housing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bayesian classifier function\n",
    "def bayesian_classifier(train_df, test_df, features):\n",
    "    \n",
    "    prior_yes = len(train_df[train_df['y'] == 'yes']) / len(train_df)\n",
    "    prior_no = len(train_df[train_df['y'] == 'no']) / len(train_df)\n",
    "\n",
    "    \n",
    "    likelihood_yes = {}\n",
    "    likelihood_no = {}\n",
    "\n",
    "    for feature in features:\n",
    "        likelihood_yes[feature] = {}\n",
    "        likelihood_no[feature] = {}\n",
    "        for value in train_df[feature].unique():\n",
    "            count_yes = len(train_df[(train_df[feature] == value) & (train_df['y'] == 'yes')])\n",
    "            count_no = len(train_df[(train_df[feature] == value) & (train_df['y'] == 'no')])\n",
    "            likelihood_yes[feature][value] = count_yes / len(train_df[train_df['y'] == 'yes'])\n",
    "            likelihood_no[feature][value] = count_no / len(train_df[train_df['y'] == 'no'])\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        evidence_yes = prior_yes\n",
    "        evidence_no = prior_no\n",
    "        for feature in features:\n",
    "            value = row[feature]\n",
    "            if value in likelihood_yes[feature]:\n",
    "                evidence_yes *= likelihood_yes[feature][value]\n",
    "            else:\n",
    "                evidence_yes *= 0.000001 \n",
    "            if value in likelihood_no[feature]:\n",
    "                evidence_no *= likelihood_no[feature][value]\n",
    "            else:\n",
    "                evidence_no *= 0.000001 \n",
    "        if evidence_yes > evidence_no:\n",
    "            predictions.append('yes')\n",
    "        else:\n",
    "            predictions.append('no')\n",
    "\n",
    "    \n",
    "    accuracy = sum(predictions == test_df['y']) / len(test_df)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decision tree classifier function\n",
    "class Node:\n",
    "    def __init__(self, feature=None, prediction=None, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.prediction = prediction\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        elif data[self.feature] == 0:\n",
    "            return self.left.predict(data)\n",
    "        else:\n",
    "            return self.right.predict(data)\n",
    "\n",
    "def decision_tree_classifier(data, features):\n",
    "    \n",
    "    if len(data['y'].unique()) == 1:\n",
    "        return Node(prediction=data['y'].iloc[0])\n",
    "    \n",
    "    if len(features) == 0:\n",
    "        prediction = data['y'].value_counts().idxmax()\n",
    "        return Node(prediction=prediction)\n",
    "    \n",
    "    best_feature = None\n",
    "    best_gain = 0\n",
    "    for feature in features:\n",
    "        gain = information_gain(data, feature)\n",
    "        if gain > best_gain:\n",
    "            best_feature = feature\n",
    "            best_gain = gain\n",
    "    \n",
    "    left_data = data[data[best_feature] == 0]\n",
    "    right_data = data[data[best_feature] == 1]\n",
    "    \n",
    "    if len(left_data) == 0:\n",
    "        prediction = data['y'].value_counts().idxmax()\n",
    "        return Node(prediction=prediction)\n",
    "    elif len(right_data) == 0:\n",
    "        prediction = data['y'].value_counts().idxmax()\n",
    "        return Node(prediction=prediction)\n",
    "    \n",
    "    left_subtree = decision_tree_classifier(left_data, set(features) - set([best_feature]))\n",
    "    right_subtree = decision_tree_classifier(right_data, set(features) - set([best_feature]))\n",
    "\n",
    "    return Node(feature=best_feature, left=left_subtree, right=right_subtree)\n",
    "\n",
    "def information_gain(data, feature):\n",
    "    \n",
    "    entropy_parent = entropy(data)\n",
    "    \n",
    "    left_data = data[data[feature] == 0]\n",
    "    entropy_left = entropy(left_data)\n",
    "    \n",
    "    right_data = data[data[feature] == 1]\n",
    "    entropy_right = entropy(right_data)\n",
    "    \n",
    "    gain = entropy_parent - len(left_data) / len(data) * entropy_left - len(right_data) / len(data) * entropy_right\n",
    "    return gain\n",
    "\n",
    "def entropy(data):\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    \n",
    "    proportion_positive = len(data[data['y'] == 'yes']) / len(data)\n",
    "    proportion_negative = len(data[data['y'] == 'no']) / len(data)\n",
    "    \n",
    "    if proportion_positive == 0 or proportion_negative == 0:\n",
    "        entropy = 0\n",
    "    else:\n",
    "        entropy = -proportion_positive * np.log2(proportion_positive) - proportion_negative * np.log2(proportion_negative)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian classifier accuracy: 0.90\n",
      "Decision tree classifier accuracy: 0.89\n",
      "The Bayesian classifier is more accurate.\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the Bayesian classifier\n",
    "bayesian_accuracy = bayesian_classifier(train_df, test_df, features)\n",
    "print(f'Bayesian classifier accuracy: {bayesian_accuracy:.2f}')\n",
    "\n",
    "# Compute the accuracy of the decision tree classifier\n",
    "features = set(features)\n",
    "decision_tree = decision_tree_classifier(train_df, features)\n",
    "decision_tree_accuracy = sum(test_df.apply(decision_tree.predict, axis=1) == test_df['y']) / len(test_df)\n",
    "print(f'Decision tree classifier accuracy: {decision_tree_accuracy:.2f}')\n",
    "\n",
    "if bayesian_accuracy > decision_tree_accuracy:\n",
    "    print('The Bayesian classifier is more accurate.')\n",
    "else:\n",
    "    print('The decision tree classifier is more accurate.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
